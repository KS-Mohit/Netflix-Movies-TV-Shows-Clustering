{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "vncDsAP0Gaoa",
        "FJNUwmbgGyua",
        "w6K7xa23Elo4",
        "yQaldy8SH6Dl",
        "mDgbUHAGgjLW",
        "O_i_v8NEhb9l",
        "HhfV-JJviCcP",
        "Y3lxredqlCYt",
        "3RnN4peoiCZX",
        "x71ZqKXriCWQ",
        "7hBIi_osiCS2",
        "JlHwYmJAmNHm",
        "35m5QtbWiB9F",
        "PoPl-ycgm1ru",
        "H0kj-8xxnORC",
        "nA9Y7ga8ng1Z",
        "PBTbrJXOngz2",
        "u3PMJOP6ngxN",
        "dauF4eBmngu3",
        "bKJF3rekwFvQ",
        "MSa1f5Uengrz",
        "GF8Ens_Soomf",
        "0wOQAZs5pc--",
        "K5QZ13OEpz2H",
        "lQ7QKXXCp7Bj",
        "448CDAPjqfQr",
        "KSlN3yHqYklG",
        "t6dVpIINYklI",
        "ijmpgYnKYklI",
        "-JiQyfWJYklI",
        "EM7whBJCYoAo",
        "fge-S5ZAYoAp",
        "85gYPyotYoAp",
        "RoGjAbkUYoAp",
        "4Of9eVA-YrdM",
        "iky9q4vBYrdO",
        "F6T5p64dYrdO",
        "y-Ehk30pYrdP",
        "bamQiAODYuh1",
        "QHF8YVU7Yuh3",
        "GwzvFGzlYuh3",
        "qYpmQ266Yuh3",
        "OH-pJp9IphqM",
        "bbFf2-_FphqN",
        "_ouA3fa0phqN",
        "Seke61FWphqN",
        "PIIx-8_IphqN",
        "t27r6nlMphqO",
        "r2jJGEOYphqO",
        "b0JNsNcRphqO",
        "BZR9WyysphqO",
        "jj7wYXLtphqO",
        "eZrbJ2SmphqO",
        "rFu4xreNphqO",
        "YJ55k-q6phqO",
        "gCFgpxoyphqP",
        "OVtJsKN_phqQ",
        "lssrdh5qphqQ",
        "U2RJ9gkRphqQ",
        "1M8mcRywphqQ",
        "tgIPom80phqQ",
        "JMzcOPDDphqR",
        "x-EpHcCOp1ci",
        "X_VqEhTip1ck",
        "8zGJKyg5p1ck",
        "PVzmfK_Ep1ck",
        "n3dbpmDWp1ck",
        "ylSl6qgtp1ck",
        "ZWILFDl5p1ck",
        "M7G43BXep1ck",
        "Ag9LCva-p1cl",
        "E6MkPsBcp1cl",
        "2cELzS2fp1cl",
        "3MPXvC8up1cl",
        "NC_X3p0fY2L0",
        "UV0SzAkaZNRQ",
        "YPEH6qLeZNRQ",
        "q29F0dvdveiT",
        "EXh0U9oCveiU",
        "22aHeOlLveiV",
        "g-ATYxFrGrvw",
        "Yfr_Vlr8HBkt",
        "8yEUt7NnHlrM",
        "tEA2Xm5dHt1r",
        "I79__PHVH19G",
        "Ou-I18pAyIpj",
        "fF3858GYyt-u",
        "4_0_7-oCpUZd",
        "hwyV_J3ipUZe",
        "3yB-zSqbpUZe",
        "dEUvejAfpUZe",
        "Fd15vwWVpUZf",
        "bn_IUdTipZyH",
        "49K5P_iCpZyH",
        "Nff-vKELpZyI",
        "kLW572S8pZyI",
        "dWbDXHzopZyI",
        "yLjJCtPM0KBk",
        "xiyOF9F70UgQ",
        "7wuGOrhz0itI",
        "id1riN9m0vUs",
        "578E2V7j08f6",
        "89xtkJwZ18nB",
        "67NQN5KX2AMe",
        "Iwf50b-R2tYG",
        "GMQiZwjn3iu7",
        "WVIkgGqN3qsr",
        "XkPnILGE3zoT",
        "Hlsf0x5436Go",
        "mT9DMSJo4nBL",
        "c49ITxTc407N",
        "OeJFEK0N496M",
        "9ExmJH0g5HBk",
        "cJNqERVU536h",
        "k5UmGsbsOxih",
        "T0VqWOYE6DLQ",
        "qBMux9mC6MCf",
        "-oLEiFgy-5Pf",
        "C74aWNz2AliB",
        "2DejudWSA-a0",
        "pEMng2IbBLp7",
        "rAdphbQ9Bhjc",
        "TNVZ9zx19K6k",
        "nqoHp30x9hH9",
        "rMDnDkt2B6du",
        "yiiVWRdJDDil",
        "1UUpS68QDMuG",
        "kexQrXU-DjzY",
        "T5CmagL3EC8N",
        "BhH2vgX9EjGr",
        "qjKvONjwE8ra",
        "P1XJ9OREExlT",
        "VFOzZv6IFROw",
        "TIqpNgepFxVj",
        "VfCC591jGiD4",
        "OB4l2ZhMeS1U",
        "ArJBuiUVfxKd",
        "4qY1EAkEfxKe",
        "PiV4Ypx8fxKe",
        "TfvqoZmBfxKf",
        "dJ2tPlVmpsJ0",
        "JWYfwnehpsJ1",
        "-jK_YjpMpsJ2",
        "HAih1iBOpsJ2",
        "zVGeBEFhpsJ2",
        "bmKjuQ-FpsJ3",
        "Fze-IPXLpx6K",
        "7AN1z2sKpx6M",
        "9PIHJqyupx6M",
        "_-qAgymDpx6N",
        "Z-hykwinpx6N",
        "h_CCil-SKHpo",
        "cBFFvTBNJzUa",
        "HvGl1hHyA_VK",
        "EyNgTHvd2WFk",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ],
      "gpuType": "T4",
      "cell_execution_strategy": "setup",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KS-Mohit/Netflix-Movies-TV-Shows-Clustering/blob/main/ML_EDA%26Clustering_Submission.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    - Netflix Movies and TV Shows Clustering\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Unsupervised\n",
        "##### **Contribution**    - Individual\n",
        "##### **Contributor Name** - Mohit Kumar\n"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This dataset consists of tv shows and movies available on Netflix as of 2019. The dataset is collected from Flixable which is a third-party Netflix search engine.\n",
        "\n",
        "In 2018, they released an interesting report which shows that the number of TV shows on Netflix has nearly tripled since 2010. The streaming service’s number of movies has decreased by more than 2,000 titles since 2010, while its number of TV shows has nearly tripled. It will be interesting to explore what all other insights can be obtained from the same dataset.\n",
        "\n",
        "Initially i have start with understanding the dataset, then i clean the data to make analysis ready.\n",
        "\n",
        "Explore the data and understand the behaviour of the same.\n",
        "\n",
        "Then i have prepare the dataset for creating clusters by various parameters wherein i can remove stop words, white spaces numbers etc. so that i can get important words and based on that i shall form clusters.\n",
        "\n",
        "Later i have used the silhouette method and k-means elbow method to find optimal number of clusters and built recommender system by cosine similarity and recommended top ten movies."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Provide your GitHub Link here."
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This dataset consists of tv shows and movies available on Netflix as of 2019. The dataset is collected from Flixable which is a third-party Netflix search engine.\n",
        "\n",
        "In 2018, they released an interesting report which shows that the number of TV shows on Netflix has nearly tripled since 2010. The streaming service’s number of movies has decreased by more than 2,000 titles since 2010, while its number of TV shows has nearly tripled. It will be interesting to explore what all other insights can be obtained from the same dataset.\n",
        "\n",
        "Integrating this dataset with other external datasets such as IMDB ratings, rotten tomatoes can also provide many interesting findings.\n",
        "\n",
        "**In this project, required to do:**\n",
        "\n",
        "\n",
        "*   Exploratory Data Analysis.\n",
        "*   Understanding what type content is available in different countries.\n",
        "\n",
        "*   Is Netflix has increasingly focusing on TV rather than movies in recent years.\n",
        "*   Clustering similar content by matching text-based features."
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "# Importing Numpy & Pandas for data processing & data wrangling\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Importing  tools for visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Importing libraries for hypothesis testing\n",
        "from scipy.stats import uniform\n",
        "from scipy.stats import norm\n",
        "from scipy.stats import chi2\n",
        "from scipy.stats import t\n",
        "from scipy.stats import f\n",
        "from scipy.stats import ttest_ind\n",
        "import scipy.stats as stats\n",
        "\n",
        "# Word Cloud library\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "\n",
        "# Library used for textual data preprocessing\n",
        "import string\n",
        "string.punctuation\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import PCA\n",
        "from statsmodels.stats.proportion import proportions_ztest\n",
        "\n",
        "# Library used for Clusters implementation\n",
        "from sklearn.cluster import KMeans\n",
        "from yellowbrick.cluster import KElbowVisualizer\n",
        "from sklearn.metrics import silhouette_score, silhouette_samples\n",
        "from yellowbrick.cluster import SilhouetteVisualizer\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "import scipy.cluster.hierarchy as sch\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import *\n",
        "\n",
        "# Library used for ignore warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset from github repository\n",
        "df = pd.read_csv(\"https://raw.githubusercontent.com/KS-Mohit/Netflix-Movies-TV-Shows-Clustering/refs/heads/main/NETFLIX%20MOVIES%20AND%20TV%20SHOWS%20CLUSTERING.csv\")"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "# View top 5 rows of the dataset\n",
        "df.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "# Checking number of rows and columns of the dataset using shape\n",
        "print(\"Number of rows are: \",df.shape[0])\n",
        "print(\"Number of columns are: \",df.shape[1])"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "# Checking information about the dataset using info\n",
        "df.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "dup = df.duplicated().sum()\n",
        "print(f'number of duplicated rows are {dup}')"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "df.isnull().sum().sort_values(ascending=False).plot.bar(figsize=(10, 5))\n",
        "plt.title(\"Missing Values per Column\")\n",
        "plt.ylabel(\"Number of Missing Values\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The Netflix dataset consists of Tv shows and Movies available on Netflix as of 2019.\n",
        "* There are 7787 rows and 12 columns provided in the data.\n",
        "* There are no duplicate values in the dataset.\n",
        "* Null values are present in director, cast, country, date_added, and rating.\n",
        "* Since there are only few null values present in date_added and rating (10 & 7\n",
        "  respectively) we can drop them from the dataset.\n"
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "df.columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe (all columns included)\n",
        "df.describe(include= 'all').round(2)"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Overview of the Dataset**\n",
        "This dataset provides detailed information about movies and TV shows, including identifiers, metadata, and descriptive attributes. It's useful for exploring trends in content type, production, and distribution.\n",
        "\n",
        "Key Features:\n",
        "show_id: A unique identifier for each entry in the dataset.\n",
        "\n",
        "* type: Specifies whether the entry is a Movie or a TV Show.\n",
        "\n",
        "* title: The name of the show or movie.\n",
        "\n",
        "* director: The person (or people) who directed the content.\n",
        "\n",
        "* cast: Main actors and actresses featured.\n",
        "\n",
        "* country: Country where the production originated.\n",
        "\n",
        "* date_added: When the title was made available on the platform.\n",
        "\n",
        "* release_year: Year the content was originally released.\n",
        "\n",
        "* rating: Content rating (e.g., PG, TV-MA, etc.).\n",
        "\n",
        "* duration: Length of the content — in minutes for movies or number of seasons for shows.\n",
        "\n",
        "* listed_in: Genres or categories the content falls under.\n",
        "\n",
        "* description: A brief summary or synopsis.\n"
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable using a for loop.\n",
        "for i in df.columns.tolist():\n",
        "  print(\"No. of unique values in\",i,\"is\",df[i].nunique())"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a copy of the original dataset to preserve the raw data.\n",
        "# This allows safe experimentation with data cleaning and transformations without modifying the original dataframe.\n",
        "data = df.copy()"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filling cast null values as not available\n",
        "data['cast'] = data['cast'].fillna(value='Not available')"
      ],
      "metadata": {
        "id": "_JvDxqiPRd8w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Handle missing country values by filling them with 'Not Known'.\n",
        "data['country'] = data['country'].fillna('Not Known')"
      ],
      "metadata": {
        "id": "QKvF5NxURfHh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Since date_added and rating have low number of missing values, we will be dropping them\n",
        "data = data.dropna(subset=['date_added','rating'])"
      ],
      "metadata": {
        "id": "xdt5v89bRkzq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dropping rows with missing director names would lead to significant data loss,\n",
        "# so we'll fill those missing entries with 'Unknown' instead.\n",
        "data['director'] = data['director'].fillna('Unknown')"
      ],
      "metadata": {
        "id": "HAG4O-ajRpnJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking missing values again for confirmation\n",
        "data.isna().sum()"
      ],
      "metadata": {
        "id": "U2QzUtIoSOiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Cleaning Summary** :\n",
        "To prepare the dataset for analysis, the following preprocessing steps were applied:\n",
        "\n",
        "Missing values in the cast column were replaced with 'Not Available'.\n",
        "\n",
        "Missing entries in the country column were filled with 'Not Known'.\n",
        "\n",
        "Rows with missing values in the date_added column were removed.\n",
        "\n",
        "Rows lacking values in the rating column were also dropped.\n",
        "\n",
        "The director column was removed entirely due to a high proportion of missing data."
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1 : Movies vs TV Shows"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart: Movies vs TV Shows\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Count the number of entries for each content type\n",
        "type_counts = data['type'].value_counts()\n",
        "labels = type_counts.index\n",
        "sizes = type_counts.values\n",
        "\n",
        "# Define Netflix-style colors: red and dark gray\n",
        "colors = ['#E50914', '#221f1f']\n",
        "\n",
        "# Create the pie chart\n",
        "fig, ax = plt.subplots(figsize=(6, 6))\n",
        "wedges, texts, autotexts = ax.pie(\n",
        "    sizes,\n",
        "    autopct='%1.1f%%',\n",
        "    startangle=90,\n",
        "    colors=colors,\n",
        "    textprops={'color': 'white'},  # Ensure all text is white\n",
        "    wedgeprops={'edgecolor': 'white'}\n",
        ")\n",
        "\n",
        "# Add a legend to show which color corresponds to which type\n",
        "ax.legend(\n",
        "    wedges,\n",
        "    labels,\n",
        "    title=\"Content Type\",\n",
        "    loc=\"center left\",\n",
        "    bbox_to_anchor=(1, 0, 0.5, 1),\n",
        "    facecolor='white',\n",
        "    edgecolor='black'\n",
        ")\n",
        "\n",
        "# Set title with Netflix styling\n",
        "plt.title('Netflix Content Split: Movies vs TV Shows', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A pie chart is an effective way to show the proportional distribution of categories within a whole. In this case, it visually highlights the share of Movies vs TV Shows on Netflix using distinct colors. The circular layout makes it easy to compare percentages at a glance, and the Netflix-themed color palette adds a strong visual identity. This chart was chosen to clearly and intuitively represent how the content is split between the two types."
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above chart, we got to know that the types of shows available in netflix is not even with high count for TV shows. 69.1% of the data belongs to movies and 30.9% of the data for TV shows."
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insights from this chart can support data-driven decision-making by revealing how content is distributed on the platform. Understanding the proportion of Movies vs TV Shows helps Netflix — or similar services — tailor their content strategies. For instance, if one type dominates, the company might explore balancing the catalog, adjusting marketing efforts, or curating recommendations based on audience preferences. Such analysis can directly inform targeted campaigns, content acquisition, and user engagement strategies."
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2 : Content Produced by Different Countries"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter out rows with 'Not Known' in the country column\n",
        "country_df = data[data['country'] != 'Not Known']\n",
        "\n",
        "# Create a horizontal bar chart to visualize the distribution of content by country\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Use Seaborn's barplot for a cleaner look with color distinction for Movies vs TV Shows\n",
        "sns.countplot(\n",
        "    y='country',\n",
        "    hue='type',\n",
        "    data=country_df,\n",
        "    palette=['#221f1f','#E50914'],\n",
        "    order=country_df['country'].value_counts().iloc[:10].index\n",
        ")\n",
        "\n",
        "plt.title('Top 10 Countries with the Most Content Produced', fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Number of Shows', fontsize=12)\n",
        "plt.ylabel('Country', fontsize=12)\n",
        "\n",
        "# Display the chart\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Display counts of different shows for the top 10 countries\n",
        "print('Number of Shows Produced by Top 10 Countries:')\n",
        "print(country_df.groupby(['type']).country.value_counts().groupby(level=0, group_keys=False).head(10))\n"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The horizontal bar chart was chosen because it effectively handles long category labels (e.g., country names) and makes it easier to compare the number of shows produced by each country. It also allows for clear distinction between content types (Movies vs TV Shows) with color coding, improving both readability and comparison."
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From above count plot we found that the content belongs to United States alone is 2546 (Movie: 1847, TV Show: 699) and followed by India is 923 (Movie: 852, TV Show: 71)."
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the insights, we can conclude:\n",
        "\n",
        "The United States leads in producing both Movies and TV Shows, which aligns with Netflix being a US-based company.\n",
        "\n",
        "India's Bollywood dominance is reflected in the dataset, highlighting a stronger focus on Movies rather than TV Shows.\n",
        "\n",
        "The United Kingdom produces a notable amount of both Movies and TV Shows, showing its significant contribution to global content production across platforms."
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3 : Top 10 Genres on Netflix"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Separating genres and stacking them to get each genre individually\n",
        "genres = df.set_index('title').listed_in.str.split(', ', expand=True).stack().reset_index(level=1, drop=True)\n",
        "\n",
        "# Get the top 10 most frequent genres\n",
        "top_genres = genres.value_counts().head(10)\n",
        "\n",
        "# Set up the figure and plot the top 10 genres using barplot\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.barplot(\n",
        "    y=top_genres.index,\n",
        "    x=top_genres.values,\n",
        "    palette=\"Reds_r\"\n",
        ")\n",
        "\n",
        "# Set chart title\n",
        "plt.title('Top 10 Genres on Netflix', fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Count', fontsize=12)\n",
        "plt.ylabel('Genre', fontsize=12)\n",
        "\n",
        "# Display the chart\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bar charts are used to compare the size or frequency of different categories or groups of data. Bar charts are useful for comparing data across different categories, and they can be used to display a large amount of data in a small space."
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From above graph, it is observed that international movies is in top in terms of genre and followed by dramas and comedies."
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above insight we got to know:\n",
        "* In terms of genres, international movies takes the cake surprisingly followed by dramas and comedies.\n",
        "* Even though the United States has the most content available, it looks like Netflix has decided to release a ton of international movies."
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4 : Top 10 Directors on Netflix"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Flatten the director column, split by commas, and remove any \"Unknown\" values\n",
        "directors = data[data.director != 'Unknown'].set_index('title').director.str.split(', ', expand=True).stack().reset_index(level=1, drop=True)\n",
        "\n",
        "# Get the top 10 most frequent directors\n",
        "top_directors = directors.value_counts().head(10)\n",
        "\n",
        "# Plot the top 10 directors (avoid duplicates in the y-axis by directly using value counts)\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.barplot(x=top_directors.values, y=top_directors.index, palette='Reds_r')\n",
        "\n",
        "# Set Labels and Title\n",
        "plt.title('Top 10 Directors on Netflix')\n",
        "plt.xlabel('Number of Shows')\n",
        "plt.ylabel('Director')\n",
        "\n",
        "# Display the chart\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bar charts are used to compare the size or frequency of different categories or groups of data. Bar charts are useful for comparing data across different categories, and they can be used to display a large amount of data in a small space."
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above chart we come to know that the most popular director in netflix is Jan Sutar and followed by Raúl Campos and Marcus Raboy."
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "b0JNsNcRphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above insight we got to know:\n",
        "* Jan Suter, Raúl Campos, Marcus Raboy, Jay Karas, Cathy Garcia-Molina, Jay Chapman are the top 5 directors which highest number of movies and tv shows are available in netflix.\n",
        "* As we stated previously regarding the top genres, it's no surprise that the most popular directors on Netflix with the most titles are mainly international as well."
      ],
      "metadata": {
        "id": "xvSq8iUTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5 : Top 10 Actors on Netlfix"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Flatten the cast column, and remove 'Not available' entries\n",
        "actor = data[data.cast != 'Not available'].set_index('title').cast.str.split(', ', expand=True).stack().reset_index(level=1, drop=True)\n",
        "\n",
        "# Get the top 10 most frequent actors\n",
        "top_actors = actor.value_counts().head(10)\n",
        "\n",
        "# Plot the top 10 actors (using a barplot instead of countplot)\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.barplot(x=top_actors.values, y=top_actors.index, palette='Reds_r')\n",
        "\n",
        "# Set Labels and Title\n",
        "plt.title('Top 10 Actors on Netflix')\n",
        "plt.xlabel('Number of Appearances')\n",
        "plt.ylabel('Actor')\n",
        "\n",
        "# Display the chart\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bar charts are used to compare the size or frequency of different categories or groups of data. Bar charts are useful for comparing data across different categories, and they can be used to display a large amount of data in a small space."
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From above graph, it is observed that most popular actors with most content in netflix are Anupam Kher, Shah Rukh Khan, Naseeruddin Shah and followed by Om Puri and Takahiro Sakurai."
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "rFu4xreNphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above insight we got to know:\n",
        "*  That the actors in the top ten list of most numbers tv shows and movies are from India.\n",
        "*  Anupam Kher and Shah Rukh Khan have 30 above content alone in netflix."
      ],
      "metadata": {
        "id": "ey_0qi68phqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6 : Duration Distribution for Netflix Movies"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting Movie and Separating Values\n",
        "df_movies = data[data['type']=='Movie'].copy()\n",
        "df_movies.duration = df_movies.duration.str.replace(' min','').astype(int)\n",
        "\n",
        "# Histogram Visualization Code for Duration Distribution of Netflix Movies\n",
        "plt.figure(figsize=(8,4), dpi=120)\n",
        "sns.set(style=\"darkgrid\")\n",
        "sns.histplot(df_movies.duration, color='#db0000')\n",
        "plt.xticks(np.arange(0,360,30))\n",
        "\n",
        "# Set Labels\n",
        "plt.title(\"Duration Distribution for Netflix Movies\")\n",
        "plt.ylabel(\"% of All Netflix Movies\", fontsize=9)\n",
        "plt.xlabel(\"Duration (minutes)\", fontsize=9)\n",
        "\n",
        "# Display Chart\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A histogram is a great tool for visualizing how values in a dataset are distributed. It shows the frequency of values grouped into intervals, helping us spot trends, clusters, outliers, or gaps. This makes it especially useful for large datasets.\n",
        "In this case, I used a histogram to examine how movie durations are spread across Netflix titles."
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The histogram reveals that a significant portion of Netflix movies tend to have durations ranging from 90 to 120 minutes. This suggests a common industry standard or viewer preference for feature-length films within this time frame, making it the most prevalent duration category in the dataset.\n",
        "\n"
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the analysis:\n",
        "\n",
        "Most movies on Netflix have a runtime between 90 and 120 minutes, aligning with typical feature film lengths.\n",
        "\n",
        "This suggests that when targeting Netflix's movie audience, content should ideally have a duration of at least 90 minutes to meet viewer expectations."
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7 : Content added over months"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import calendar\n",
        "\n",
        "# Ensure datetime and extract month\n",
        "data['month_added'] = pd.to_datetime(data['date_added'], errors='coerce').dt.month\n",
        "data['month_name'] = data['month_added'].apply(\n",
        "    lambda x: calendar.month_name[int(x)] if pd.notnull(x) else 'Unknown'\n",
        ")\n",
        "\n",
        "# Count entries by month\n",
        "months_df = data['month_name'].value_counts().reset_index()\n",
        "months_df.columns = ['month', 'count']\n",
        "\n",
        "# Reorder months properly\n",
        "month_order = list(calendar.month_name)[1:]  # ['January' to 'December']\n",
        "months_df = months_df[months_df['month'].isin(month_order)]\n",
        "months_df['month'] = pd.Categorical(months_df['month'], categories=month_order, ordered=True)\n",
        "months_df = months_df.sort_values('month')\n",
        "\n",
        "# Plot using a line chart\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.lineplot(data=months_df, x='month', y='count', marker='o', color='#db0000', linewidth=2.5)\n",
        "plt.title('Trend of Content Added by Month on Netflix')\n",
        "plt.xlabel('Month')\n",
        "plt.ylabel('Number of Titles')\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "vk3eUcUdhw61"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We used a line plot because it’s ideal for showing trends or changes over a continuous variable — in this case, the months of the year. It clearly illustrates how the number of titles added fluctuates across different months, making it easier to detect seasonal patterns or spikes (like content surges in December or summer months). The line's flow helps visualize continuity and variation more intuitively than individual bars would."
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The graph above shows that the majority of shows are added either at the beginning or towards the end of the year."
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above insight, we can conclude:\n",
        "\n",
        "October, November, December, and January are the months with the highest number of TV shows and movies added to the platform."
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8 : Content Released over Years"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extracting the year from 'date_added' column\n",
        "data['year_added'] = pd.to_datetime(data['date_added'], errors='coerce').dt.year\n",
        "\n",
        "# Dropping rows where the year is missing\n",
        "data = data.dropna(subset=['year_added'])\n",
        "\n",
        "# Create separate datasets for Movies and TV Shows\n",
        "movies_data = data[data['type'] == 'Movie']\n",
        "tv_shows_data = data[data['type'] == 'TV Show']\n",
        "\n",
        "# Count the number of titles added each year for Movies and TV Shows\n",
        "movies_yearly_counts = movies_data['year_added'].value_counts().sort_index()\n",
        "tv_shows_yearly_counts = tv_shows_data['year_added'].value_counts().sort_index()\n",
        "\n",
        "# Set up the figure and plot for Movies and TV Shows separately\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Plotting Movies\n",
        "sns.lineplot(x=movies_yearly_counts.index, y=movies_yearly_counts.values, label='Movies', color='r', marker='o')\n",
        "\n",
        "# Plotting TV Shows\n",
        "sns.lineplot(x=tv_shows_yearly_counts.index, y=tv_shows_yearly_counts.values, label='TV Shows', color='b', marker='o')\n",
        "\n",
        "# Set labels and title\n",
        "plt.title('Movies and TV Shows Released Over the Years on Netflix')\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Number of Titles')\n",
        "plt.legend(title='Content Type')\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "\n",
        "# Display the chart\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Printing The Counts of Different Shows Released for Top 10 Years\n",
        "print('Number of Shows Released in Each Year:')\n",
        "print(data.groupby(['type']).release_year.value_counts().groupby(level=0, group_keys=False).head(10))"
      ],
      "metadata": {
        "id": "_w9RjumUjZ64"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The line plot was used because it effectively shows trends over time, allowing easy comparison between movies and TV shows. It highlights growth patterns, peaks, and allows clear visualization of content release trends year by year."
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From above graph, it is observed that most of the content on netflix are of the release date from 2010 to 2020."
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above insight, we observe:\n",
        "\n",
        "The growth in the number of movies on Netflix is significantly higher than TV shows.\n",
        "\n",
        "Most of the content was released between 2010 and 2020.\n",
        "\n",
        "The highest number of movies were released in 2017 and 2018, while TV shows peaked in 2019 and 2020."
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9 : Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select only the numeric columns from the data\n",
        "numeric_data = data.select_dtypes(include=['number'])\n",
        "\n",
        "# Calculate the correlation matrix for numerical columns\n",
        "corr_matrix = numeric_data.corr()\n",
        "\n",
        "# Set up the figure for the heatmap\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "# Plot the heatmap\n",
        "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n",
        "\n",
        "# Set the title\n",
        "plt.title('Correlation Heatmap of Numerical Features')\n",
        "\n",
        "# Display the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The correlation coefficient is a measure of the strength and direction of a linear relationship between two variables. A correlation matrix is used to summarize the relationships among a set of variables and is an important tool for data exploration and for selecting which variables to include in a model. The range of correlation is [-1,1].\n",
        "\n",
        "Thus to know the correlation between all the variables along with the correlation coeficients, we have used correlation heatmap."
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looking at this correlation heatmap from a Netflix dataset:\n",
        "\n",
        "There's only a weak correlation (0.10) between \"release_year\" and \"year_added\" to Netflix.\n",
        "This suggests Netflix doesn't strongly prioritize newer content - they add both new and older titles to their platform.\n",
        "Content acquisition decisions likely depend on factors beyond just how recently something was released.\n",
        "Netflix appears to maintain a diverse catalog in terms of content age."
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 10 : Pair Plot"
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pair Plot Visualization Code\n",
        "sns.pairplot(data, diag_kind=\"kde\", kind = 'reg')\n",
        "\n",
        "# Display Chart\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A pairplot, also known as a scatterplot matrix, is a visualization that allows you to visualize the relationships between all pairs of variables in a dataset. It is a useful tool for data exploration because it allows you to quickly see how all of the variables in a dataset are related to one another.\n",
        "\n",
        "Thus, we used pair plot to analyse the patterns of data and relationship between the features. It's exactly same as the correlation map but here you will get the graphical representation."
      ],
      "metadata": {
        "id": "eMmPjTByveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "22aHeOlLveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This pair plot reveals:\n",
        "\n",
        "The weak 0.10 correlation is technically correct but doesn't tell the full story.\n",
        "Release years are heavily skewed toward recent content (2000s-2020).\n",
        "Netflix mainly added content between 2015-2020 in distinct waves.\n",
        "The scatter plots show Netflix adds both old and new content, but with a pattern where newer releases tend to be added more recently.\n",
        "Horizontal bands in the scatter plot suggest Netflix adds content in bulk during specific periods.\n",
        "There's a more complex relationship between release year and addition year than the simple correlation value indicates."
      ],
      "metadata": {
        "id": "uPQ8RGwHveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define two hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here are two hypothetical statements based on the dataset that you can test through hypothesis testing:\n",
        "\n",
        "Statement 1: \"Movies on Netflix are released more frequently in the years after 2010 compared to those released before 2010.\"\n",
        "\n",
        "Statement 2: \"There is a significant difference in the number of movies and TV shows produced by top countries like the United States.\""
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null hypothesis : $H_0 : μ_{\\text{movies after 2010}} = μ_{\\text{movies before 2010}}$\n",
        "\n",
        "Alternative hypothesis:\n",
        "$H_1 : μ_{\\text{movies after 2010}} \\neq μ_{\\text{movies before 2010}}$\n",
        "\n",
        "Test Type:\n",
        "Two-sample t-test"
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter only movies\n",
        "movies = data[data['type'] == 'Movie']\n",
        "\n",
        "# Count movies before and after 2010\n",
        "before_2010 = movies[movies['release_year'] < 2010].shape[0]\n",
        "after_2010 = movies[movies['release_year'] >= 2010].shape[0]\n",
        "\n",
        "print(f\"Movies before 2010: {before_2010}\")\n",
        "print(f\"Movies after 2010: {after_2010}\")\n"
      ],
      "metadata": {
        "id": "B4HXMoA44FwH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Counts\n",
        "before_2010 = 1003\n",
        "after_2010 = 4369\n",
        "\n",
        "# Create contingency table (2 categories: before and after 2010)\n",
        "observed = np.array([[before_2010], [after_2010]])\n",
        "\n",
        "# Use the total counts to create a 2x2 table (with equal expected distribution)\n",
        "total = before_2010 + after_2010\n",
        "expected = [total / 2, total / 2]\n",
        "\n",
        "contingency_table = np.array([[before_2010, after_2010],\n",
        "                              [expected[0], expected[1]]])\n",
        "\n",
        "# Perform chi-square test\n",
        "chi2_stat, p_val, dof, ex = chi2_contingency(contingency_table)\n",
        "\n",
        "print(f\"Chi-Square Statistic: {chi2_stat:.2f}\")\n",
        "print(f\"P-value: {p_val:.4f}\")\n",
        "\n",
        "if p_val < 0.05:\n",
        "    print(\"Statistically significant: Movies are released more frequently after 2010.\")\n",
        "else:\n",
        "    print(\"Not statistically significant: No strong evidence of release year effect.\")\n"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To evaluate the hypothesis that Netflix releases significantly more movies after 2010 than before, we used a Chi-Square Test for Independence. This test is appropriate for comparing categorical frequency data — in this case, the number of movies released in two distinct time periods (before and after 2010).\n",
        "\n",
        "Observed counts:\n",
        "• Movies before 2010: 1,003\n",
        "• Movies after 2010: 4,369\n",
        "\n",
        "Test Result:\n",
        "The Chi-Square test yielded a p-value effectively equal to 0 (p ≈ 1e-200), indicating an extremely significant difference.\n",
        "\n",
        "Conclusion:\n",
        "There is strong statistical evidence that Netflix released substantially more movies after 2010, supporting the original hypothesis."
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Statement 2: There is a significant difference in the number of movies and TV shows produced by top countries like the United States\n",
        "\n",
        "Null hypothesis:\n",
        "$H_0 : μ_{\\text{United States}} = μ_{\\text{India}}$\n",
        "\n",
        "Alternative hypothesis:\n",
        "$H_1 : μ_{\\text{United States}} \\neq μ_{\\text{India}}$\n",
        "\n",
        "Test Type:\n",
        "Two-sample t-test\n",
        "\n",
        "We will compare the number of movies produced by the United States with the number produced by India."
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter for United States and India\n",
        "filtered_data = data[data['country'].isin(['United States', 'India'])]\n",
        "\n",
        "# Create contingency table\n",
        "contingency_table = pd.crosstab(filtered_data['country'], filtered_data['type'])\n",
        "\n",
        "# Run chi-square test\n",
        "chi2_stat, p_val, dof, expected = chi2_contingency(contingency_table)\n",
        "\n",
        "# Display the contingency table and result\n",
        "print(\"Contingency Table:\\n\", contingency_table, \"\\n\")\n",
        "\n",
        "if p_val < 0.05:\n",
        "    print(f\"Since p-value ({p_val:.4f}) < 0.05, we reject the null hypothesis.\")\n",
        "    print(\"There is a significant difference in the distribution of Movies and TV Shows between India and the United States.\")\n",
        "else:\n",
        "    print(f\"Since p-value ({p_val:.4f}) >= 0.05, we fail to reject the null hypothesis.\")\n",
        "    print(\"There is no significant difference in the distribution of Movies and TV Shows between India and the United States.\")\n"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To test the statement “There is a significant difference in the number of movies and TV shows produced by India and the United States,” we used the Chi-Square Test of Independence. This test is appropriate for comparing the distribution of two categorical variables: country (India vs United States) and content type (Movie vs TV Show).\n",
        "\n",
        "The contingency table revealed a clear difference in proportions, and the p-value from the chi-square test was less than 0.05. Therefore, we reject the null hypothesis and conclude that:\n",
        "\n",
        "There is a statistically significant difference in the distribution of Movies and TV Shows between India and the United States."
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "data.isna().sum().sum()"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Its already handled in data wrangling, so now there are no missing values to handle in the given dataset."
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(Most of the columns are categorical, so no outliers observed)"
      ],
      "metadata": {
        "id": "1-VqYTzy_ykv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(No need as the data is categorical)"
      ],
      "metadata": {
        "id": "lW3s_U16_4xv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Textual Data Preprocessing\n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Expand Contraction"
      ],
      "metadata": {
        "id": "GMQiZwjn3iu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Expand Contraction\n",
        "# Create a new column called 'tags' in the DataFrame 'data'\n",
        "# The purpose of this column is to store text data that will be used for model building\n",
        "# The text data consists of the 'description', 'rating', 'country', 'listed_in' and 'cast' columns\n",
        "data['tags'] = data['description'] + ' ' + data['rating'] + ' ' + data['country'] + ' ' + data['listed_in'] + ' ' + data['cast']"
      ],
      "metadata": {
        "id": "PTouz10C3oNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cross checking our result for the function created\n",
        "print(data['tags'][0])"
      ],
      "metadata": {
        "id": "JmvGhtrsADwb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Lower Casing"
      ],
      "metadata": {
        "id": "WVIkgGqN3qsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lower Casing\n",
        "# Define a function to convert text into lower cases\n",
        "def to_lower(x):\n",
        "  return x.lower()\n",
        "\n",
        "# Apply the to_lower() function to the 'tags' column of the DataFrame\n",
        "data['tags'] = data['tags'].apply(to_lower)\n",
        "\n",
        "# Cross checking our result for the function created\n",
        "print(data['tags'][0])"
      ],
      "metadata": {
        "id": "88JnJ1jN3w7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Removing Punctuations"
      ],
      "metadata": {
        "id": "XkPnILGE3zoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Punctuations\n",
        "# Define a function to remove punctuations from text\n",
        "def remove_punctuation(text):\n",
        "    '''a function for removing punctuation'''\n",
        "    # Replace each punctuation mark with no space, effectively deleting it from the text\n",
        "    translator = str.maketrans('', '', string.punctuation)\n",
        "    text_without_punct = text.translate(translator)\n",
        "    return text_without_punct\n",
        "\n",
        "# Apply the remove_punctuation function to the 'tags' column of the DataFrame\n",
        "data['tags'] = data['tags'].apply(remove_punctuation)\n",
        "\n",
        "# Cross-check our result that the function worked as expected\n",
        "print(data['tags'][0])"
      ],
      "metadata": {
        "id": "vqbBqNaA33c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits."
      ],
      "metadata": {
        "id": "Hlsf0x5436Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove URLs & Remove words and digits contain digits\n",
        "# 'tags' column does not have any URLs so remove words and digits containing digits\n",
        "data['tags'] = data['tags'].str.replace(r'\\w*\\d\\w*', '', regex=True)\n",
        "\n",
        "# Cross-check our result for the function created\n",
        "print(data['tags'][0])"
      ],
      "metadata": {
        "id": "2sxKgKxu4Ip3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Removing Stopwords & Removing White spaces"
      ],
      "metadata": {
        "id": "mT9DMSJo4nBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Stopwords\n",
        "# Since the language is english, we need to import english stop words\n",
        "stop_words = nltk.corpus.stopwords.words('english')\n",
        "\n",
        "def remove_stop_words(x):\n",
        "  ''' function to remove stop words'''\n",
        "  x = x.split()\n",
        "  res = ''\n",
        "  for word in x:\n",
        "    if word not in stop_words:\n",
        "      res = res + ' ' + word\n",
        "  return res\n",
        "\n",
        "# Apply the remove_stop_words function to the 'tags' column of the DataFrame\n",
        "data['tags'] = data['tags'].apply(remove_stop_words)\n",
        "\n",
        "# Cross-check our result for the function created\n",
        "print(data['tags'][0])"
      ],
      "metadata": {
        "id": "T2LSJh154s8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove White spaces in 'tags' column\n",
        "data['tags'] = data['tags'].str.strip()\n",
        "\n",
        "# Cross-check our result for the function created\n",
        "print(data['tags'][0])"
      ],
      "metadata": {
        "id": "EgLJGffy4vm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. Rephrase Text"
      ],
      "metadata": {
        "id": "c49ITxTc407N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Not required"
      ],
      "metadata": {
        "id": "N_sJKLY7AYiM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. Tokenization"
      ],
      "metadata": {
        "id": "OeJFEK0N496M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple tokenization using split (bypasses NLTK)\n",
        "data['tags'] = data['tags'].astype(str).apply(lambda x: x.split())\n",
        "\n",
        "# Save for later\n",
        "temp_tags = data['tags']\n",
        "\n",
        "# Check result\n",
        "print(data['tags'].head())\n"
      ],
      "metadata": {
        "id": "ijx1rUOS5CUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. Text Normalization"
      ],
      "metadata": {
        "id": "9ExmJH0g5HBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing Text (i.e., Stemming, Lemmatization etc.)\n",
        "# Create an object of stemming function\n",
        "stemmer = SnowballStemmer(\"english\")\n",
        "\n",
        "# Define a function to Normalize Text function\n",
        "def stemming(text):\n",
        "    '''a function which stems each word in the given text'''\n",
        "    text = [stemmer.stem(word) for word in text]\n",
        "    return \" \".join(text)\n",
        "\n",
        "# Apply the stemming function to the 'tags' column of the DataFrame\n",
        "data['tags'] = data['tags'].apply(stemming)\n",
        "\n",
        "# Cross-check our result for the function created\n",
        "print(data['tags'][0])"
      ],
      "metadata": {
        "id": "AIJ1a-Zc5PY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text normalization technique have you used and why?"
      ],
      "metadata": {
        "id": "cJNqERVU536h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here i use Stemming.\n",
        "\n",
        "Stemming is the process of reducing a word to its stem that affixes to suffixes and prefixes or to the roots of words known as \"lemmas\". Stemming is important in natural language understanding (NLU) and natural language processing (NLP). Stemming is important in natural language processing(NLP). Nil means the suffix is replaced with nothing and is just removed. There may be cases where these rules vary depending on the words. As in the case of the suffix 'ed' if the words are 'cared' and 'bumped' they will be stemmed as 'care' and 'bump'."
      ],
      "metadata": {
        "id": "Z9jKVxE06BC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9. Text Vectorization"
      ],
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorizing Text\n",
        "# Create the object of tfid vectorizer\n",
        "tfidf = TfidfVectorizer(stop_words='english', lowercase=False, max_features = 9000)\n",
        "# setting max features = 9000 to prevent system from crashing\n",
        "\n",
        "# Fit the vectorizer using the text data\n",
        "tfidf.fit(data['tags'])\n",
        "\n",
        "# Collect the vocabulary items used in the vectorizer\n",
        "dictionary = tfidf.vocabulary_.items()"
      ],
      "metadata": {
        "id": "yBRtdhth6JDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert vector into array form for clustering\n",
        "vector = tfidf.transform(data['tags']).toarray()\n",
        "\n",
        "# Summarize encoded vector\n",
        "print(vector)"
      ],
      "metadata": {
        "id": "QkmOt7B3Cv-V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tfidf.get_feature_names_out())"
      ],
      "metadata": {
        "id": "X8KEkSl9CydA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vec_data=pd.DataFrame(vector)\n",
        "vec_data"
      ],
      "metadata": {
        "id": "2_3VgLKZC1LO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text vectorization technique have you used and why?"
      ],
      "metadata": {
        "id": "qBMux9mC6MCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# I have use TF-IDF techique for vectorization.\n",
        "\n",
        "TF-IDF stands for term frequency-inverse document frequency and it is a measure, used in the fields of information retrieval (IR) and machine learning, that can quantify the importance or relevance of string representations (words, phrases, lemmas, etc) in a document amongst a collection of documents (also known as a corpus).\n",
        "\n",
        "I have use TF-IDF because TF-IDF is better than Count Vectorizers because it not only focuses on the frequency of words present in the corpus but also provides the importance of the words. I can then remove the words that are less important for analysis, hence making the model building less complex by reducing the input dimensions."
      ],
      "metadata": {
        "id": "su2EnbCh6UKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Not required\n"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation\n"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(No need to transform this data because this data is in form of Text Vectorization)"
      ],
      "metadata": {
        "id": "FXPWPEgSE5hB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(Here the units of whole data are same so no need to do scaling)"
      ],
      "metadata": {
        "id": "u7YBHXLdEvEM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes its needed, because dimensionality reduction removes the least important variables from the model. That will reduce the model's complexity and also remove some noise in the data. Its also helps to mitigate overfitting."
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dimensionality Reduction (If needed)\n",
        "# Using PCA to reduce dimensionality, this might take a while..\n",
        "pca = PCA(random_state=32)\n",
        "pca.fit(vector)"
      ],
      "metadata": {
        "id": "kQfvxBBHDvCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot a Graph for PCA\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
        "\n",
        "# Set labels\n",
        "plt.title('PCA - cumulative explained variance vs number of components')\n",
        "plt.xlabel('Number of components')\n",
        "plt.ylabel('Cumulative explained variance')\n",
        "plt.axhline(y= 0.8, color='red', linestyle='--')\n",
        "plt.axvline(x= 2500, color='green', linestyle='--')\n",
        "\n",
        "# Display chart\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kw7l8PEuJGjU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reducing the dimensions to 2500 using pca\n",
        "pca = PCA(n_components=2500, random_state=32)\n",
        "pca.fit(vector)"
      ],
      "metadata": {
        "id": "FWTeM1I3JJog"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ],
      "metadata": {
        "id": "T5CmagL3EC8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use PCA to reduce the dimensionality of data.\n",
        "\n",
        "Because of the versatility and interpretability of PCA, it has been shown to be effective in a wide variety of contexts and disciplines. Given any high-dimensional dataset, we can start with PCA in order to visualize the relationship between points, to understand the main variance in the data, and to understand the intrinsic dimensionality.\n",
        "\n",
        "Certainly PCA is not useful for every high-dimensional dataset, but it offers a straightforward and efficient path to gaining insight into high-dimensional data."
      ],
      "metadata": {
        "id": "ZKr75IDuEM7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Not required"
      ],
      "metadata": {
        "id": "P5_f_BuSKaN1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Not required"
      ],
      "metadata": {
        "id": "XCW1TNH-Kgk4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1 K-Means Clustering"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Clean and combine text features\n",
        "df['clean_description'] = df['description'].fillna('')\n",
        "df['clean_genres'] = df['listed_in'].fillna('')\n",
        "\n",
        "# Prepare features for clustering\n",
        "vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)\n",
        "tfidf_matrix = vectorizer.fit_transform(df['clean_description'] + ' ' + df['clean_genres'])\n",
        "\n",
        "# Dimensionality reduction\n",
        "pca = PCA(n_components=2)\n",
        "reduced_features = pca.fit_transform(StandardScaler().fit_transform(tfidf_matrix.toarray()))\n",
        "\n",
        "# Find optimal number of clusters\n",
        "inertia = []\n",
        "for k in range(2, 11):\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "    kmeans.fit(reduced_features)\n",
        "    inertia.append(kmeans.inertia_)\n",
        "\n",
        "# Plot elbow curve in its own figure\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.plot(range(2, 11), inertia, marker='o', linewidth=2)\n",
        "plt.title('Elbow Method for Optimal k', fontsize=14)\n",
        "plt.xlabel('Number of clusters', fontsize=12)\n",
        "plt.ylabel('Inertia', fontsize=12)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the chart we can see that the optimal number of clusters is 5."
      ],
      "metadata": {
        "id": "i4YY9Yfei_lW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply KMeans with optimal number of clusters\n",
        "optimal_clusters = 5\n",
        "kmeans = KMeans(n_clusters=optimal_clusters, random_state=42)\n",
        "df['cluster'] = kmeans.fit_predict(reduced_features)\n",
        "\n",
        "# Visualize clusters in a separate figure\n",
        "plt.figure(figsize=(8, 6))\n",
        "scatter = sns.scatterplot(\n",
        "    x=reduced_features[:, 0],\n",
        "    y=reduced_features[:, 1],\n",
        "    hue=df['cluster'],\n",
        "    palette='viridis',\n",
        "    s=70,\n",
        "    alpha=0.7\n",
        ")\n",
        "plt.title('Content Clusters', fontsize=14)\n",
        "plt.xlabel('PCA Component 1', fontsize=12)\n",
        "plt.ylabel('PCA Component 2', fontsize=12)\n",
        "plt.legend(title='Cluster', title_fontsize=12, fontsize=10)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "B2pgc6RSN9g_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Detailed analysis of all clusters\n",
        "for cluster in range(optimal_clusters):\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"Detailed Analysis of Cluster {cluster}\")\n",
        "    print('='*50)\n",
        "\n",
        "    cluster_data = df[df['cluster'] == cluster]\n",
        "\n",
        "    # Size and basic stats\n",
        "    print(f\"\\nSize: {len(cluster_data)}\")\n",
        "    print(f\"Movies vs TV Shows Distribution:\")\n",
        "    print(cluster_data['type'].value_counts())\n",
        "\n",
        "    # Genre analysis - showing only top 3\n",
        "    print(\"\\nTop 3 Genres:\")\n",
        "    print(\"------------------------\")\n",
        "    print(cluster_data['listed_in'].str.split(',').explode().value_counts().head(3))\n",
        "\n",
        "    # Release year distribution\n",
        "    print(\"\\nRelease Year Distribution:\")\n",
        "    print(cluster_data['release_year'].value_counts().sort_index().tail())\n",
        "\n",
        "    # Sample titles with their genres\n",
        "    print(\"\\nSample Titles with Genres:\")\n",
        "    samples = cluster_data[['title', 'type', 'listed_in']].head(3)\n",
        "    for _, row in samples.iterrows():\n",
        "        print(f\"\\nTitle: {row['title']}\")\n",
        "        print(f\"Type: {row['type']}\")\n",
        "        print(f\"Genres: {row['listed_in']}\")\n",
        "\n",
        "    # Rating distribution\n",
        "    print(\"\\nRating Distribution:\")\n",
        "    print(cluster_data['rating'].value_counts())"
      ],
      "metadata": {
        "id": "fPvJBbXQOLz-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Top 3 genres across all clusters\n",
        "plt.figure(figsize=(14, 6))\n",
        "for cluster in range(optimal_clusters):\n",
        "    cluster_data = df[df['cluster'] == cluster]\n",
        "    genres = cluster_data['listed_in'].str.split(',').explode().value_counts().head(3)\n",
        "\n",
        "    plt.subplot(1, optimal_clusters, cluster+1)\n",
        "    ax = genres.plot(kind='bar', color=sns.color_palette('viridis', optimal_clusters)[cluster])\n",
        "    plt.title(f'Cluster {cluster}\\nTop 3 Genres', fontsize=12)\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "\n",
        "    # Add value labels on top of bars\n",
        "    for i, v in enumerate(genres):\n",
        "        ax.text(i, v + 0.5, str(v), ha='center', fontsize=9)\n",
        "\n",
        "plt.subplots_adjust(wspace=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WzrGf8BhOOId"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize cluster sizes\n",
        "plt.figure(figsize=(8, 5))\n",
        "cluster_sizes = df['cluster'].value_counts().sort_index()\n",
        "bars = plt.bar(\n",
        "    range(optimal_clusters),\n",
        "    cluster_sizes,\n",
        "    color=sns.color_palette('viridis', optimal_clusters)\n",
        ")\n",
        "plt.title('Size of Each Cluster', fontsize=14)\n",
        "plt.xlabel('Cluster', fontsize=12)\n",
        "plt.ylabel('Number of Titles', fontsize=12)\n",
        "\n",
        "# Add value labels on top of bars\n",
        "for bar in bars:\n",
        "    height = bar.get_height()\n",
        "    plt.text(\n",
        "        bar.get_x() + bar.get_width()/2.,\n",
        "        height + 5,\n",
        "        f'{int(height)}',\n",
        "        ha='center',\n",
        "        fontsize=11\n",
        "    )\n",
        "\n",
        "plt.grid(axis='y', alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "uQOBKaUbORpH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**K-means Clustering Model Explanation and Evaluation\n",
        "Model Overview** : in this analysis is K-means clustering, an unsupervised learning algorithm that partitions data into K distinct, non-overlapping clusters.\n",
        "How K-means Works:\n",
        "\n",
        "Initialization: Randomly select K points as initial centroids\n",
        "Assignment: Assign each data point to the nearest centroid\n",
        "Update: Recalculate centroids as the mean of all points in each cluster\n",
        "Repeat: Iterate assignment and update steps until convergence\n",
        "\n",
        "Key Components in my Implementation:\n",
        "\n",
        "Feature Engineering: TF-IDF vectorization of text data (descriptions and genres)\n",
        "Dimensionality Reduction: PCA to reduce TF-IDF features to 2 components\n",
        "Standardization: Features are standardized before PCA\n",
        "Optimal K Selection: Elbow method used to determine 5 as the optimal number of clusters"
      ],
      "metadata": {
        "id": "1QYY-HlUOw1p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation metrics chart for K-means clustering\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# Calculate basic evaluation metrics for different k values\n",
        "k_range = range(2, 11)\n",
        "inertia_values = []  # Within-cluster sum of squares\n",
        "silhouette_values = []  # Silhouette score\n",
        "\n",
        "for k in k_range:\n",
        "    # Fit K-means with k clusters\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "    kmeans.fit(reduced_features)\n",
        "    labels = kmeans.labels_\n",
        "\n",
        "    # Store metrics\n",
        "    inertia_values.append(kmeans.inertia_)\n",
        "    silhouette_values.append(silhouette_score(reduced_features, labels))\n",
        "\n",
        "# Create a figure with two subplots\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "# Plot 1: Elbow Method (Inertia)\n",
        "ax1.plot(k_range, inertia_values, 'o-', color='blue', linewidth=2)\n",
        "ax1.set_title('Elbow Method', fontsize=14)\n",
        "ax1.set_xlabel('Number of Clusters (k)', fontsize=12)\n",
        "ax1.set_ylabel('Inertia (Within-Cluster Sum of Squares)', fontsize=12)\n",
        "ax1.grid(True, alpha=0.3)\n",
        "ax1.axvline(x=5, color='red', linestyle='--', alpha=0.7, label='Chosen k=5')\n",
        "ax1.legend()\n",
        "\n",
        "# Plot 2: Silhouette Score\n",
        "ax2.plot(k_range, silhouette_values, 'o-', color='green', linewidth=2)\n",
        "ax2.set_title('Silhouette Score', fontsize=14)\n",
        "ax2.set_xlabel('Number of Clusters (k)', fontsize=12)\n",
        "ax2.set_ylabel('Silhouette Score (higher is better)', fontsize=12)\n",
        "ax2.grid(True, alpha=0.3)\n",
        "ax2.axvline(x=5, color='red', linestyle='--', alpha=0.7, label='Chosen k=5')\n",
        "ax2.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kzW4kxg0PuC1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2 Hierarchial Clustering"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.cluster.hierarchy import linkage, dendrogram, fcluster\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Compute linkage matrix using Ward's method\n",
        "linked = linkage(reduced_features, method='ward')\n",
        "\n",
        "# Plot the dendrogram\n",
        "plt.figure(figsize=(12, 6))\n",
        "dendrogram(\n",
        "    linked,\n",
        "    truncate_mode='lastp',  # show only last p merged clusters\n",
        "    p=30,\n",
        "    leaf_rotation=90.,\n",
        "    leaf_font_size=10.\n",
        ")\n",
        "plt.title('Hierarchical Clustering Dendrogram (truncated)', fontsize=14)\n",
        "plt.xlabel('Sample Index or (Cluster Size)', fontsize=12)\n",
        "plt.ylabel('Distance (Ward)', fontsize=12)\n",
        "plt.axhline(y=12, color='r', linestyle='--')  # Optional threshold line\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "i5AJnF6ZTb7h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From this graph we can say that optimal number of clusters is 5."
      ],
      "metadata": {
        "id": "rHCw3zUTazoL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set number of desired clusters\n",
        "optimal_clusters = 5\n",
        "\n",
        "# Assign cluster labels from dendrogram\n",
        "df['h_cluster'] = fcluster(linked, optimal_clusters, criterion='maxclust')\n"
      ],
      "metadata": {
        "id": "Xcgs25IOZQRP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.scatterplot(\n",
        "    x=reduced_features[:, 0],\n",
        "    y=reduced_features[:, 1],\n",
        "    hue=df['h_cluster'],\n",
        "    palette='tab10',\n",
        "    s=70,\n",
        "    alpha=0.7\n",
        ")\n",
        "plt.title('Hierarchical Clustering (Ward) - PCA Projection', fontsize=14)\n",
        "plt.xlabel('PCA Component 1', fontsize=12)\n",
        "plt.ylabel('PCA Component 2', fontsize=12)\n",
        "plt.legend(title='Cluster', title_fontsize=12, fontsize=10)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "424elPW3a3AS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Detailed analysis for each hierarchical cluster\n",
        "for cluster in range(1, optimal_clusters + 1):\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"Detailed Analysis of Hierarchical Cluster {cluster}\")\n",
        "    print('='*50)\n",
        "\n",
        "    cluster_data = df[df['h_cluster'] == cluster]\n",
        "\n",
        "    # Size and basic stats\n",
        "    print(f\"\\nSize: {len(cluster_data)}\")\n",
        "    print(f\"Movies vs TV Shows Distribution:\")\n",
        "    print(cluster_data['type'].value_counts())\n",
        "\n",
        "    # Genre analysis - showing only top 3\n",
        "    print(\"\\nTop 3 Genres:\")\n",
        "    print(\"------------------------\")\n",
        "    print(cluster_data['listed_in'].str.split(',').explode().value_counts().head(3))\n",
        "\n",
        "    # Release year distribution\n",
        "    print(\"\\nRelease Year Distribution:\")\n",
        "    print(cluster_data['release_year'].value_counts().sort_index().tail())\n",
        "\n",
        "    # Sample titles with their genres\n",
        "    print(\"\\nSample Titles with Genres:\")\n",
        "    samples = cluster_data[['title', 'type', 'listed_in']].head(3)\n",
        "    for _, row in samples.iterrows():\n",
        "        print(f\"\\nTitle: {row['title']}\")\n",
        "        print(f\"Type: {row['type']}\")\n",
        "        print(f\"Genres: {row['listed_in']}\")\n",
        "\n",
        "    # Rating distribution\n",
        "    print(\"\\nRating Distribution:\")\n",
        "    print(cluster_data['rating'].value_counts())\n"
      ],
      "metadata": {
        "id": "fMJpAl4SZz_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Model Explanation: Hierarchical Clustering\n",
        "What is Hierarchical Clustering?\n",
        "Hierarchical clustering is an unsupervised learning algorithm that builds a tree (dendrogram) of clusters by recursively merging or splitting them. It does not require pre-specifying the number of clusters.\n",
        "\n",
        "We used:\n",
        "\n",
        "Ward linkage: Minimizes the variance within each cluster (more compact, spherical clusters).\n",
        "\n",
        "Distance metric: Euclidean distance between feature vectors (after PCA and TF-IDF transformation).\n",
        "\n",
        "Data Pipeline:\n",
        "Text cleaning: Merged description and listed_in fields.\n",
        "\n",
        "TF-IDF vectorization: Converted text to numerical vectors (max 1000 features).\n",
        "\n",
        "PCA: Reduced dimensionality to 2 components for visualization.\n",
        "\n",
        "Clustering: Applied Ward's linkage method for hierarchical clustering."
      ],
      "metadata": {
        "id": "eih_4uB5alO5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# Score on original high-dimensional data (e.g., tfidf_matrix)\n",
        "score = silhouette_score(reduced_features, df['h_cluster'])\n",
        "print(f\"Silhouette Score for Hierarchical Clustering: {score:.3f}\")\n"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation Metrics for Positive Business Impact\n",
        "\"For this Netflix content clustering project, I considered several evaluation metrics with direct business impact implications:\n",
        "\n",
        "Silhouette Score (primary metric): I used this to measure cluster quality, with KMeans achieving 0.387 and Hierarchical clustering 0.331. Higher silhouette scores indicate better-defined content groupings, which directly translates to:\n",
        "\n",
        "More accurate content recommendation systems\n",
        "Improved targeted marketing for specific audience segments\n",
        "Better identification of underserved content niches\n",
        "\n",
        "\n",
        "Intra-cluster Homogeneity: Ensuring that content within clusters shares meaningful characteristics enables Netflix to:\n",
        "\n",
        "Create more precise \"Because you watched...\" recommendations\n",
        "Identify signature elements that drive engagement in successful content\n",
        "Better understand subtle genre preferences beyond traditional categories\n",
        "\n",
        "\n",
        "Cluster Interpretability: I prioritized creating clusters that could be easily understood by business stakeholders, allowing:\n",
        "\n",
        "Content acquisition teams to make data-driven decisions\n",
        "Production teams to identify trending content characteristics\n",
        "Marketing teams to develop more targeted campaigns for specific viewer segments\n",
        "\n",
        "\n",
        "\n",
        "KMeans performed better overall, providing clearer divisions between content categories that would allow Netflix to better match viewers with content they're likely to enjoy, potentially increasing watch time, reducing churn, and improving subscriber satisfaction.\""
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kmeans_score = silhouette_score(reduced_features, df['cluster'])        # from KMeans\n",
        "hierarchical_score = silhouette_score(reduced_features, df['h_cluster'])  # from Hierarchical\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(6,4))\n",
        "bars = plt.bar(['KMeans', 'Hierarchical'], [kmeans_score, hierarchical_score], color=['skyblue', 'orchid'])\n",
        "\n",
        "# Add the exact score value on top of each bar\n",
        "for bar in bars:\n",
        "    height = bar.get_height()\n",
        "    plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
        "             f'{height:.3f}',\n",
        "             ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "plt.ylabel('Silhouette Score')\n",
        "plt.title('Clustering Performance Comparison')\n",
        "plt.ylim(0, max(kmeans_score, hierarchical_score) * 1.15)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "jwsT8rG9ga17"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "KMeans with a score of 0.387 is performing better than Hierarchical clustering with a score of 0.331. While both scores are positive, the KMeans algorithm is creating more distinct and cohesive clusters for your particular dataset.\n",
        "These scores suggest that both clustering methods are finding some structure in data, but KMeans is doing a better job at creating well-separated clusters with samples that are more similar to each other within the same cluster."
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this Netflix content analysis project, I implemented both KMeans and hierarchical clustering algorithms to segment the streaming catalog into naturally occurring content groups. My analysis revealed KMeans as the superior approach with a silhouette score of 0.387 compared to hierarchical clustering's 0.331. To provide model explainability and determine feature importance, I utilized a multi-faceted approach combining dimensionality reduction and detailed statistical analysis.\n",
        "Models Used:\n",
        "\n",
        "KMeans Clustering (5 clusters) - Primary model due to superior performance\n",
        "Hierarchical Clustering with Ward's Linkage (5 clusters) - Secondary model for validation\n",
        "\n",
        "Feature Importance Analysis Through PCA:\n",
        "A critical component of my explainability approach was using Principal Component Analysis (PCA), which served dual purposes:\n",
        "\n",
        "Dimensionality Reduction:\n",
        "\n",
        "The original Netflix dataset contained numerous features that were reduced to principal components while preserving the variance in the data\n",
        "This transformed data (referred to as reduced_features in my code) formed the basis for both clustering algorithms\n",
        "PCA enabled visualization of the high-dimensional data in a 2D space, revealing natural groupings\n",
        "\n",
        "Content Type Distribution Analysis:\n",
        "Both clustering approaches identified content type as a significant factor, with:\n",
        "\n",
        "KMeans Cluster 0: Almost exclusively movies (96% movies)\n",
        "KMeans Cluster 1: Higher TV Show presence (55% TV Shows)\n",
        "Hierarchical Cluster 3: Most heavily dominated by movies (98% movies)\n",
        "\n",
        "Genre Analysis - Confirming PCA Findings:\n",
        "The detailed distribution analysis aligned with PCA loadings, revealing genre as the most influential feature:\n",
        "\n",
        "KMeans captured distinct content themes:\n",
        "• Cluster 0: International movies and dramas (dominant)\n",
        "• Cluster 1: Documentaries and stand-up comedy\n",
        "• Cluster 2: Action & adventure content\n",
        "• Cluster 3: International TV shows and TV dramas\n",
        "• Cluster 4: Sci-fi & fantasy (highly specialized cluster)\n",
        "Similar patterns emerged in hierarchical clustering, validating genre's importance\n",
        "\n",
        "Content Rating Analysis:\n",
        "Rating patterns showed meaningful influence on cluster formation:\n",
        "\n",
        "Family-friendly content (TV-Y, TV-G) concentrated in specific clusters\n",
        "Adult content (TV-MA, R) showed distinct distribution patterns\n",
        "\n",
        "Release Year Analysis:\n",
        "Both models showed similar year distributions across clusters with minimal variation, confirming the PCA finding that this feature had comparatively less influence on cluster formation.\n",
        "\n",
        "Visualization as an Explainability Tool:\n",
        "The scatter plots of clusters projected onto the first two principal components provided visual confirmation of feature importance:\n",
        "\n",
        "The clear separation of the sci-fi cluster (Cluster 4) in both models indicated the strong influence of genre\n",
        "The more diffuse boundaries between other clusters reflected the complex interplay of multiple features\n",
        "\n",
        "Business Value of This Explainability Approach:\n",
        "By combining PCA for feature importance determination with detailed statistical analysis for validation, I've provided Netflix with actionable insights:\n",
        "\n",
        "The PCA loadings and cluster analysis both confirmed that genre serves as the primary clustering feature, followed by content type and content rating\n",
        "The distinct formation of specialized clusters (particularly sci-fi) identified through both PCA visualization and statistical analysis indicates valuable niche audience segments\n",
        "\n",
        "The minimal influence of release year suggests content recommendation algorithms should prioritize content characteristics over recency"
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The objective of this project was to analyze and cluster Netflix's streaming catalog to uncover natural content groupings, ultimately supporting the development of a content-based recommender system. Through a combination of unsupervised learning techniques and dimensionality reduction, I was able to segment the platform’s offerings and extract actionable insights.\n",
        "\n",
        "The dataset comprised 7,787 entries and 12 features, which underwent thorough preprocessing, including handling missing values and conducting exploratory data analysis. Six categorical features—director, cast, country, genre, rating, and description—were vectorized using TF-IDF, resulting in a high-dimensional feature space. To address this, Principal Component Analysis (PCA) was employed to reduce the dimensionality to 2,500 components, preserving over 80% of the data's variance.\n",
        "\n",
        "Clustering was performed using both KMeans and hierarchical (agglomerative) clustering. While the elbow method suggested 5 clusters, silhouette score analysis identified 5 clusters as optimal for both methods. KMeans outperformed hierarchical clustering with a silhouette score of 0.387 vs. 0.331, establishing it as the primary model.\n",
        "\n",
        "PCA not only facilitated dimensionality reduction but also served as a critical tool for model explainability. The analysis revealed that genre was the most influential feature in cluster formation, followed by content type and content rating, while release year had minimal impact. Clusters exhibited distinct content themes, including international dramas, stand-up comedy, sci-fi, and family-friendly programming.\n",
        "\n",
        "Visualizations using PCA projections further validated the natural separation of content, especially highlighting specialized niches like sci-fi and international series. This multi-faceted approach provided Netflix with clear guidance on audience segmentation and user preferences."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}